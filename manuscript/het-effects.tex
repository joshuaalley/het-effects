\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{graphicx, rotating, booktabs} 
\usepackage{times} 
\usepackage{fbb} 
\usepackage{natbib} 
\usepackage{indentfirst} 
\usepackage{setspace}
\usepackage{grffile} 
\usepackage{hyperref}
\usepackage{tikz-cd}
 \usetikzlibrary{cd}
\usepackage[export]{adjustbox}
\usepackage[most]{tcolorbox}
\usepackage{verbatimbox}
\usepackage{lscape}
\usepackage{afterpage}
\usepackage{amsmath}
\usepackage[labelfont={bf},textfont=it,labelsep=period]{caption}
 \usepackage{multirow} 
\setcitestyle{aysep{}}
\usepackage{dcolumn}

\hypersetup{
  colorlinks = true,
  urlcolor = blue,
  linkcolor = black,
  citecolor = black,
  pdfauthor = {Joshua Alley},
  pdfkeywords = {},
  pdftitle = {},
  pdfsubject = {},
  pdfpagemode = UseNone,
%  pdffitwindow = true
%  pdfcenterwindow = true
}



\singlespace
\title{\textbf{Using Hierarchical Models to Estimate Heterogeneous Effects}}
\author{Joshua Alley \\
Assistant Professor \\
University College Dublin \\
joshua.alley@ucd.ie
}

 
\date{\today}

\bibliographystyle{apsr}

\usepackage{sectsty}
\sectionfont{\Large}
\subsectionfont{\noindent\large\textit}
\subsubsectionfont{\normalsize}

\makeatletter
\renewcommand\tiny{\@setfontsize\tiny{9}{10}}
\makeatother


\begin{document}

\maketitle 

\begin{abstract} 
%Heterogeneous effects are common in social science. 
This note describes how and when to use Bayesian hierarchical models to estimate heterogeneous effects. 
While an ample literature suggests that hierarchical models provide helpful regularization and information about variation, political scientists rarely use them to estimate heterogeneous effects. 
Doing so is simple, however. 
To start, specify groups based on quantities of interest such as demographics, context, and policy relevance.  
Then, fit a hierarchical model where treatment slopes and intercepts vary across groups.
This captures systematic and random variation in heterogeneous effects, estimates effects within each group, and measures effect variance. 
Hierarchical modeling provides an intermediate tool between interactions or subgroup analyses and machine learning approaches to discovering complex heterogeneity. 
It is more flexible than interactions and reduces the risk of underpowered subgroup comparisons.
At the same time, it is more theoretically informed and interpretable than some machine learning approaches, as well as easier to implement in small datasets. 
Researchers can thus use hierarchical models alongside other approaches to understand heterogeneous effects for scholarship and policy.
\end{abstract} 


\newpage 
\doublespace 


\section{Introduction}


% one: het effects matter
Whether in observational or experimental studies, every independent variable social scientists examine impacts some units differently than others. 
Common estimands aggregate heterogeneous effects.\footnote{For instance, \citet{Abramsonetal2022} note that the average marginal component effect (AMCE) of conjoint experiments gives more weight to intense preferences.} 
These average effects are useful, but they often obscure interesting and important variation. 


As a result, understanding heterogeneous effects is essential for policy and scholarship. 
Estimating heterogeneity allows scholars to clarify the connection between their independent variable and outcome.
Policymakers can maximize the impact of finite resources with targeted interventions, for example by providing job training to individuals who are more likely to benefit. 
% expand/sharpen this, if there is space. (There is not for PA) 


% two: introduce my solution 
This letter explains how and when to use hierarchical models to estimate heterogeneous effects. 
I identify when researchers can profitably use hierarchical models, and when other tools make more sense. 
A large statistics literature suggests that Bayesian hierarchical models are a useful tool for  heterogeneous effects estimation (e.g., \cite{FellerGelman2015, McElreath2016, Dorieetal2022}).
Political scientists tend to rely on interactions or machine learning tools instead, however.  
For instance, of the three applied political science citations of \citet{FellerGelman2015}, only \citet{Marquardt2022} models treatment effects. 


% three: loads of techniques
Hierarchical modeling of heterogeneous effects fills a gap between interactions and machine learning.\footnote{\citet{BlackwellOlson2022} describe a lasso approach to interactions that sits between machine learning and linear regressions.}
Parametric interactions and subgroup analyses are ubiquitous because they are easy to interpret.
These approaches are hard to interpret with more than three dimensions and are often underpowered \citep{Simmonsetal2011}.
More recent work employs random forests \citep{GreenKern2012, WagerAthey2018}, support vector machines \citep{ImaiRatkovic2013}, and ensemble methods \citep{Grimmeretal2017, Kuenzeletal2019, Dorieetal2022}.
These machine learning algorithms can discover complex patterns and high-dimensional variation, but can be difficult to interpret and implement, especially in smaller social science datasets.

 
Using a hierarchical model is more flexible than parametric interactions but easier to implement and interpret than machine learning approaches.  
It preserves a simple and interpretable structure, while accommodating more factors and ameliorating the downsides of subgroup analysis via partial pooling. 
This facilitates argument testing.
Unlike machine learning, the hierarchical approach lacks the flexibility to discover high-dimensional heterogeneity, however.  


Hierarchical modeling therefore works best when there are more than two modifying factors and therefore many subgroups of interest, as well as less emphasis on discovery. 
These models are best at capturing variation across groups and levels when there are multiple potential modifiers.
This also works best when researchers have a clear sense of the relevant groups.\footnote{\citep{Goplerud2021} introduces a model that uses Bayesian structured sparsity to estimate which group coefficients are similar and which are different. In this researchers use theory to inform the potential sets of groups, but common estimates for groups are data driven.} 


Considering multiple modifiers is important because social scientists often posit conditional theories.
Most conditional arguments consider a single modifier, but the accumulation of singe-modifier arguments suggests that multiple factors generate heterogeneous effects.
For example, scholarship on audience costs has considered how foreign policy dispositions \citep{KertzerBrutger2016}, partisanship \citep{LevenduskyHorowitz2012}, gender \citep{Barnhartetal2020, SchwartzBlair2020} and policy preferences \citep{Chaudoin2014} all modify individual reactions to a leader backing down from a threat. 



There are two key steps when theory and data make using hierarchical models worthwhile.
First, researchers should define groups based on potential sources of heterogeneity such as other treatments, context, demographics, or policy concerns. 
Second, they should estimate heterogeneous effects across those groups using a hierarchical model with varying slopes and intercepts for every unique group. 
Modeling heterogeneous effects in this way produces interpretable results, which facilitates argument testing.
It also allows researchers to examine effects within groups, compare different sources of heterogeneous effects and describe how much an effect varies.  


While frequentist estimation of hierarchical models is possible, Bayesian estimation is straightforward and more informative.
Bayesian estimation provides crucial information by connecting parameters through common prior distributions, thereby regularizing estimates and propagating uncertainty. 
Working with posterior distributions also gives researchers more flexibility to present diverse information about how and when effects vary. 
While computation and coding were once a barrier to employing Bayesian methods, fitting a wide range of hierarchical models is straightforward with the brms package in \textsf{R} \citep{Buerkner2017}.\footnote{I provide example code in this note and the appendix.}
Calculating substantive effects is also simple \citep{ArelBundockme}.



% wrap and introduce the application 
In the remainder of this note, I describe how and when to estimate hierarchical models of heterogeneous effects and demonstrate the process by analyzing a study of how military alliances shape public support for war by \citet{TomzWeeks2021}. 
The reanalysis reveals that alliances increase support for intervention most among white men who support international engagement but are otherwise skeptical of using force. 
%Alliances increase mass support for war by impacting individuals who otherwise prefer peaceful collaboration. 



\section{Hierarchical Modeling of Heterogeneous Effects}


There are two steps in hierarchical models of heterogeneous effects. 
First, researchers must define the groups over which an independent variable's impact changes. 
Unique combinations of characteristics such as other treatments, context and demographics determine groups.


Researchers should create groups based on what variation is most important and interesting. 
Theory, policy concerns, or normative factors are all possible motivations. 


Setting groups is the most important task, because it determines what heterogeneous effects a researcher estimates. 
Defining groups before model fitting defines what variation is most important, links heterogeneous effects to theory, and structures modeling.\footnote{It also facilitates pre-registration when applicable.}
Defining groups without careful thought risks obfuscating results and can hinder model fitting.   


% three ways to set groups
There are three general approaches to defining groups.  
First, researchers can set groups using combinations of other treatments, especially when an intervention has several dimensions but theory emphasizes one of them. 
The experimental design determines groups, and the model estimates heterogeneous treatment effects.   
If researchers want to know how different issues shape the impact of elite foreign policy cues \citep{GuisingerSaunders2017}, they could define groups by issues, for instance.
Hierarchical estimators for topic-sampling experiments apply this idea to estimate how a treatment varies across different topics \citep{CliffordRainey2023}. 
Researchers sometime use fully crossed interactions to estimate the impact of a treatment across experimental strata. 


A second approach uses unit, demographic and contextual factors to create groups and estimate  effect heterogeneity. 
Here, researchers examine what factors within or around units shape their response to an independent variable.
For example, \citet{Alley2021isq} uses alliance characteristics to examine when alliance membership increases or decreases military spending.
Other use cases include estimating how different demographic groups or geographic units respond to an intervention.
One might examine how the impact of a national-level intervention varied across states, for example.\footnote{Extrapolation to a representative sample for such units might require poststratification.}


Third, researchers might use hierarchical models to address specific policy concerns.
Policy analysts often want to know how an intervention impacts a specific population. 
Researchers might want to know if a job-training program improves employment outcomes for black women in the South, for instance.  


% grouping factors: numbers 
Whether researchers use other treatments, context, or policy to determine groups, the number of grouping factors depends first on theory.
There are some practical constraints, however.
Using too many factors can lead to model fitting and interpretation problems by creating many small groups.
How many factors is too many depends especially on the data- some datasets can support reasonably large groups for many factors. 
At the other extreme, using only one grouping factor will add relatively little value compared to interactions. 



After defining groups, the second step is fitting a hierarchical model of effects within groups.
The model employs a mix of varying intercepts and slopes to estimate the impact of an intervention in each group.\footnote{If other units such as states define the groups, rather than combinations of modifying variables, then adding group-level predictors is essential. For examine, in a model where an effect varies by state, adding state-level variables like ideology, population and GDP would avoid partially pooling small groups too far towards the overall mean.}
%The first equation links the independent variable and outcome.
%The second equation estimates heterogeneous effects as a function of group characteristics.
%The second equation is essential, as group variables add substantial information and avoid simply pooling small groups towards the overall mean.
%Without the group predictors, the random effects of each treatment will shift towards the mean because the information about each group depends only on how many data points it contains. 


This approach can address diverse problems, but for ease of exposition consider making between-unit comparisons based on an experimental treatment.    
Start with \textit{N} units indexed by \textit{i}, some of which receive a binary treatment \textit{T}.
Assume that the outcome variable ${y}$ is normally distributed with mean $\mu_i$ and standard deviation $\sigma$.\footnote{Researchers should use binary, categorical and other outcome likelihoods.}
\textit{g} indexes the researcher-defined groups, which include two variables \textit{v1} and \textit{v2} and their interaction.


%The first equation predicts the outcome mean. 
The outcome for each unit is then a function of group varying intercepts $\alpha_g$, an optional matrix of control variables \textbf{X}, and a set of group treatment effects $\theta_g$, which are normally distributed with mean $\eta_g$ and standard deviation $\sigma_\theta$. 
The researcher divides all units into \textit{g} groups based on unique combinations of heterogeneous effect predictors \textbf{Z}. 
Each $\theta$ parameter estimates how the treatment effect varies across the values of  each variable.  
To capture correlations between the random intercepts and varying slopes $\rho \sigma_\alpha \sigma_\theta$, these variables should have a common multivariate normal prior.


\begin{align*}
y_i &\sim N(\mu_i, \sigma) &\text{(Likelihood)} \\
\mu_i &= \alpha + \alpha_{v1} + \theta_{v1} \textit{T} + \alpha_{v2} + \theta_{v2} \textit{T} + \alpha_{v1*v2} + \theta_{v1*v2} \textit{T} + \textbf{X} \beta &\text{(Outcome Equation)}  \\ 
\begin{pmatrix} 
\alpha_{v1} \\
\theta_{v1} \\
\end{pmatrix} &\sim  N
\begin{bmatrix}
\begin{pmatrix}
\mu^\alpha_{v1} \\
\mu^\theta_{v1} \\
\end{pmatrix}\!\!,&
\begin{pmatrix}
\sigma^2_\alpha & \rho \sigma^\alpha_{v1} \sigma^\theta_{v1} \\
\rho \sigma^\alpha_{v1} \sigma^\theta_{v1} & \sigma^2_\theta \\
\end{pmatrix}
\end{bmatrix} & \text{(Common Prior: V1)} \\ 
\begin{pmatrix} 
\alpha_{v2} \\
\theta_{v2} \\
\end{pmatrix} &\sim  N
\begin{bmatrix}
\begin{pmatrix}
\mu^\alpha_{v2} \\
\mu^\theta_{v2} \\
\end{pmatrix}\!\!,&
\begin{pmatrix}
\sigma^2_\alpha & \rho \sigma^\alpha_{v2} \sigma^\theta_{v2} \\
\rho \sigma^\alpha_{v2} \sigma^\theta_{v2} & \sigma^2_\theta \\
\end{pmatrix}
\end{bmatrix} & \text{(Common Prior: V2)} \\ 
\begin{pmatrix} 
\alpha_{v1*v2} \\
\theta_{v1*v2} \\
\end{pmatrix} &\sim  N
\begin{bmatrix}
\begin{pmatrix}
\mu^\alpha_{v1*v2} \\
\mu^\theta_{v1*v2} \\
\end{pmatrix}\!\!,&
\begin{pmatrix}
\sigma^2_\alpha & \rho \sigma^\alpha_{v1*v2} \sigma^\theta_{v1*v2} \\
\rho \sigma^\alpha_{v1*v2} \sigma^\theta_{v1*v2} & \sigma^2_\theta \\
\end{pmatrix}
\end{bmatrix} & \text{(Common Prior: V1*V2)} \\ 
\theta_g &= \theta_{v1} \textit{V1} + \theta_{v2} \textit{V2} + \theta_{v1*v2} \textit{V1*V2} & \text{(Group Slopes)} \\
\alpha_g &= \alpha_{v1} + \alpha_{v2} + alpha_{v1*v2} & \text{(Group Intercepts)} \\
%\mu_\theta &= \lambda_0 + \textbf{Z} \lambda &\text{(Heterogeneous Effects)} 
\end{align*}


This approach lets slopes vary across multiple variables within a group. 
The net impact of the treatment in each group depends on the linear combination of slopes in that group, specifically the $\theta$ parameters and values of each variable. 
Similarly, the group intercepts can be calculated as the sum of the corresponding random intercepts for each grouping variable.\footnote{In brms for a model with no controls and two variables modifying the impact of a treatment, the model formula is y $\sim$ 1 + (1 + treat | var1*var2)}  


%The heterogeneous effects equation then predicts the treatment effects with the matrix \textbf{Z}, which contains unique combinations of whatever variables define the groups.  
%As a result, each $\theta$ reflects a unique mix of factors that modify the treatment.
%The second equation also includes an intercept $\lambda_0$ that estimates the impact of treatment when all sources of heterogeneity are zero.\footnote{In brms for a model with no controls and two variables modifying the impact of a treatment, the model formula is y $\sim$ 1 + treat*(var1 + var2) + (1 + treat | var1:var2). treat*(var1 + var2) expresses part of the second equation, while (1 + treat | var1:var2) lets slopes vary by group.}


The above model can be fit with Bayesian or frequentist methods, but Bayesian estimation offers important advantages.
First, it is more flexible, and including prior information can facilitate model fitting and convergence. 
Priors also help regularize estimates by pulling extreme groups towards the overall mean.
Working with posterior distributions also also provides a wealth of information about effect heterogeneity and propagates uncertainty.  


% Interpretation 
In interpreting these models, researchers should leverage the full range of information from the different parameters. 
First, the $\theta$ posteriors give the impact of a variable within each group.
All $\theta$s reflect a systematic component from the predictors in \textbf{Z}$\lambda$ and a random variation in slopes from $\sigma_\theta$. 
Unless the group-level predictors are weak correlates of treatment response, the systematic component will dominate. 
The random variation is similar to the error term in regression- it expresses how much variation is left in addition to the systematic component. 


% additional information 
In addition to group-specific effect estimates, a hierarchical model facilitates rich description of effects across groups. 
%It estimates how specific factors drive differences between groups via the $\lambda$ parameters.
Researchers can calculate variance in the $\theta$ parameters across groups and compare the posteriors of different $\theta$s. 
The $\sigma_\theta$ parameter summarizes the random variation. 
Other techniques such as interactions in OLS with robust standard errors provide less information.


\section{When to Use Hierarchical Models}

% advantages 
In deciding whether to use hierarchical models, researchers must weigh it's unique advantages and disadvantages. 
In general, estimating heterogeneous effects in this way has three advantages.
First, researchers can make detailed inferences about heterogeneous effects in an interpretable framework. 
This helps examine theories that predict how an effect varies and compare sources of variation.\footnote{Rescaling variables in the heterogeneous effects equation can aid model fitting and coefficient comparisons \citep{Gelman2008}.} 
Partial pooling also facilitates reasonable estimates for small groups by sharing information across groups and incorporating predictors in the heterogeneous effects equation. 
Finally, this approach will be faster than machine learning approaches for many datasets, easier to use in small datasets, and may scale better than models of individual treatment effects.


% disadvantages
Like all methods, the hierarchical approach has downsides, some of which can be ameliorated with modifications, while others should lead researchers to use different tools. 
Because groups are based on unique combinations of heterogeneous effect variables, using multiple continuous variables in the heterogeneous effects equation creates many small groups or individual treatment effects, which increases the risk of sampling problems, especially in small datasets. 
If using continuous variables hinders model convergence, researchers can bin continuous variables.


Furthermore, hierarchical models can show general trends, but will not make powerful comparisons between every group. 
Researchers who want to compare specific groups may lack empirical leverage, especially for small groups.


% Relative to interactions
With these considerations in mind, when should researchers use hierarchical models in place of interactions?
If only one factor modifies an effect, interactions are best, as the extra information hierarchical models provide is less valuable. 


% two modifiers
With two or more modifiers, hierarchical models begin to add value beyond interactions. 
Interpreting triple interactions between a variable and two modifiers is challenging. 
The advantages of hierarchical modeling increase with the number of modifiers, until additional modifiers create small groups that complicate model fitting. 
The thresholds where the number of modifiers becomes an issue for hierarchical modeling depends on the data, as larger datasets can support more groups. 


% not as well suited for discovery 
The relative use cases of hierarchical models and machine learning are different. 
Unlike machine learning approaches, hierarchical models will not discover high-dimensional interactions. 
Researchers can add flexibility with additional interactions or non-linear specifications in either level of the model, but this requires a priori specification. 
Therefore, if researchers want to focus on flexible discovery, not testing an argument with multiple sources of treatment heterogeneity, they should rely more on machine-learning. 


In summary, researchers should continue to use interactions for single modifiers and machine learning to discover complex interactions. 
Hierarchical modeling works well when there are two or more modifiers and researchers have adequate data to support an informative model.  
\autoref{tab:tools-det} summarizes some relevant characteristics of hierarchical, interaction and machine learning approaches to heterogeneous effects. 
Hierarchical modeling is thus an intermediate tool between interactions and machine-learning, where researchers need more flexibility than interactions but are not willing or able to tackle the computational and interpretation challenges of machine learning. 


\begin{table}
\begin{tabular}{|p{1in}|p{1.5in}|p{1.5in}|p{1.5in}|} \hline
                 & Hierarchical Models & Interactions/Subgroup & Machine Learning \\
\hline
Factors              & Two or more          & One or two         & Many \\ \hline
Sample Size          & Conditional on number of factors            & Medium to large, depending on main effect size    & Large \\ \hline
Complexity           & Medium             & Low                & High \\ \hline
Computational Cost   & Medium             & Low                & High \\ \hline
Interpretability     & High               & High               & Low \\ \hline
Modifiers            & Specified          & Specified      & Discovered or Specified \\
\hline
\end{tabular}
\caption{Key characteristics of different approaches to estimating heterogeneous effects.}
\label{tab:tools-det}
\end{table}


\section{Example Application} 


In the following, I demonstrate how the hierarchical approach works by reanalyzing a study by \citet{TomzWeeks2021} (TW hereafter). 
TW examine whether the public is more willing to go to war for an allied country.
In a factorial experiment with vignettes, they find a 33\% average increase in support for military intervention on behalf of another country if that country is an ally. 
This is a large and potentially important effect, so I estimate who responds to alliances.\footnote{See the appendix for a heterogeneous treatments analysis that corroborates TW's results.}


I used race, gender, hawkishness and internationalism to define demographic groups and predict treatment heterogeneity across those groups. 
I selected these variables because foreign policy dispositions like militant assertiveness shape general willingness to use force \citep{Kertzeretal2014} as do gender \citep{Barnhartetal2020} and race. 
I also control for other experimental manipulations.\footnote{See the appendix for priors.} 
Following TW's OLS analysis, I use a Gaussian likelihood, although the outcome is a binary variable. 


I describe the results in two steps. 
First, I summarize the distribution of alliance effects. 
After this, I summarize the sources of variation in the alliance effect in \autoref{fig:tw-het-source} and present the resulting heterogeneous effects for every group in \autoref{fig:tw-treat-het}.

\begin{figure}[htpb]
	\centering
		\includegraphics[width=0.95\textwidth]{../figures/tw-treat-het-sum.png}
	\caption{Posterior distribution of all estimated impacts of alliances on support for using force. Text values give notable point estimates, and parentheses summarize the 95\% credible interval.}
	\label{fig:tw-treat-het-sum}
\end{figure}


How alliances impact support for using force varies widely. 
\autoref{fig:tw-treat-het-sum} provides an initial summary of that variation, and highlights several noteworthy estimates. 


First, \autoref{fig:tw-treat-het-sum} notes that the minimum estimated impact of an alliance on a demographic group is .05, while the maximum is .53. 
The maximum effect occurs among white men with high internationalism and low hawkishness.
The minimum effect applies to non-white women with low internationalism and high hawkishness. 
There is no overlap in the posteriors of these estimates. 
The median group treatment effect estimate is .31, and this group of respondents is non-white men with middling internationalism and hawkishness. 
Alliances never clearly decrease support for intervention, but how much they increase support varies widely. 


\autoref{fig:tw-treat-het-sum} also presents the variation in how alliances impact demographic groups.
The standard deviation of all posterior draws is .13. 
Roughly 5\% of variation in the alliance effect is not explained by systematic regression components in \autoref{fig:tw-het-source}.\footnote{This is $\sigma_\theta$ above.}


\autoref{fig:tw-het-source} plots how the impact of alliances varies across support for international engagement, willingness to use force, race and gender. 
These variables define the groups, so differences in the alliance slope follows their individual and joint variation. 
Because there are many groups, the impact of alliances varies widely within each level of these variables, but there are some clear patterns. 


\begin{figure}[htpb]
	\centering
		\includegraphics[width=0.95\textwidth]{../figures/tw-het-source.png}
	\caption{Variation in the impact of alliances on support for military intervention across four variables that set groups. Each point marks the impact of alliances on a specific group, and boxplots summarize the median and interquartile range of the slopes within each level of the variable. All slopes are present in each facet.}
	\label{fig:tw-het-source}
\end{figure}


Individuals with minimal interest in international engagement are less responsive to alliances, while any greater support for internationalism leads to a fairly consistent response to alliances. 
Similarly, alliances exert less impact on individuals who have minimal militant assertiveness. 
Alliances are very influential for individuals with low but greater than minimal hawkishness, however. 
Among those with moderate or high hawkishness, alliances have a fairly consistent impact. 
The media alliance impact is also greater for men, and among white respondents. 


Crucially, parameteric interactions would not capture as clearly the non-linear steps across the levels of different grouping variables. 
Letting slopes vary across each level of the grouping variables generates more flexibility, and clearly shows the difference between individuals with very low internationalism or low militant assertiveness and others. 


As \autoref{fig:tw-het-source} suggests, alliances increase support for foreign intervention most among white men, especially those with low hawkishness and some internationalism.
By contrast, alliances have little impact on support for war among non-white females who are also skeptical of international engagement and unwilling to use force.
Individuals with more ambivalent foreign policy views respond more typically to TW's alliance treatment. 


\begin{figure}[htpb]
	\centering
		\includegraphics[width=0.95\textwidth]{../figures/tw-treat-het.png}
	\caption{Estimates of the impact of military alliances on support for using force within demographic groups. Column facets are values of internationalism, and row facets are levels of hawkishness. X-axis divided by gender and colors demarcate gender. Points mark the posterior median and bars summarize the 95\% credible interval.}
	\label{fig:tw-treat-het}
\end{figure}


All these estimates suggest that internationalism matters more than than hawkishness for understanding who is willing to fight for U.S. allies. 
Alliances may impact hawks less because these individuals support intervention regardless. 
Military alliances matter most to backers of international engagement who less willing to use force, but not entirely averse to military intervention.


% variation
These results show some of the strengths and weaknesses of the hierarchical approach to heterogeneous effects.\footnote{In the appendix, I analyze \citet{BushPrather2020}.}
A simple model based on demographic groups provides precise insights about who heeds alliances in supporting using force abroad. 
At the same time, because some demographic groups are small, the within-group effect estimates have substantial uncertainty and powerful comparisons between most groups is challenging. 
Fewer groups would have more data and less uncertainty but perhaps obscure variation across key demographic characteristics. 


\section{Conclusion}


This note explained how and when to use hierarchical models to estimate heterogeneous effects. 
Bayesian modeling can apply to a wide range of outcomes, data structures, and theories. 
It also details what drives variation in an effect and how much an effect varies. 
Explicitly modeling how different groups respond to an independent variable can help test arguments and inform policy.  


Hierarchical modeling provides an intermediate approach between interactions or subgroup analyses and machine learning algorithms. 
For interactions with one or two variables, relying on simple interaction tools is best. 
Similarly, machine learning is best for discovery of complex heterogeneity.
When there are two or more modifiers and many groups of theoretical interest, hierarchical modeling allows theoretically informed and interpretable estimation of effect variation. 

As a result, hierarchical modeling complements existing tools and should not replace them. 
Researchers can use hierarchical models to check and inform other techniques, for instance by seeing if a key interaction holds when there are multiple modifiers, or comparing multiple modifiers that past theories have identified. 
Using hierarchical modeling can thus help scholars and policymakers better understand heterogeneous effects.


\section*{Acknowledgements}

Thanks to Taylor Kinsley Chewning, Andrew Gelman and Carlisle Rainey for helpful comments.

\singlespace
\bibliography{../../MasterBibliography} 


%\input{../appendix/appendix.tex}

\end{document}

