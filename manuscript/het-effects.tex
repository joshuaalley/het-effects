\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{graphicx, rotating, booktabs} 
\usepackage{times} 
\usepackage{fbb} 
\usepackage{natbib} 
\usepackage{indentfirst} 
\usepackage{setspace}
\usepackage{grffile} 
\usepackage{hyperref}
\usepackage{tikz-cd}
 \usetikzlibrary{cd}
\usepackage[export]{adjustbox}
\usepackage[most]{tcolorbox}
\usepackage{verbatimbox}
\usepackage{lscape}
\usepackage{afterpage}
\usepackage{amsmath}
\usepackage[labelfont={bf},textfont=it,labelsep=period]{caption}
 \usepackage{multirow} 
\setcitestyle{aysep{}}
\usepackage{dcolumn}

\hypersetup{
  colorlinks = true,
  urlcolor = blue,
  linkcolor = black,
  citecolor = black,
  pdfauthor = {Joshua Alley},
  pdfkeywords = {},
  pdftitle = {},
  pdfsubject = {},
  pdfpagemode = UseNone,
%  pdffitwindow = true
%  pdfcenterwindow = true
}



\singlespace
\title{\textbf{Using Hierarchical Models to Estimate Heterogeneous Effects}}
\author{Joshua Alley \\
Assistant Professor \\
University College Dublin \\
joshua.alley@ucd.ie
}

 
\date{\today}

\bibliographystyle{apsr}

\usepackage{sectsty}
\sectionfont{\Large}
\subsectionfont{\noindent\large\textit}
\subsubsectionfont{\normalsize}

\makeatletter
\renewcommand\tiny{\@setfontsize\tiny{9}{10}}
\makeatother


\begin{document}

\maketitle 

\begin{abstract} 
%Heterogeneous effects are common in social science. 
This note describes why, when, and how to use Bayesian hierarchical models to estimate heterogeneous effects. 
While an ample literature suggests that hierarchical models provide helpful regularization and information about effect variation, political scientists rarely use them to estimate heterogeneous effects. 
Doing so is simple, however. 
To start, specify groups based on quantities of interest such as demographics, context, and policy relevance.  
Then, fit a hierarchical model where treatment slopes and intercepts vary across groups.
This captures systematic and random variation in heterogeneous effects, estimates effects within each group, and measures effect variance. 
Hierarchical modeling is more flexible than linear interactions and reduces the risk of underpowered subgroup comparisons.
I document these claims with a simulation analysis and extension of a published study. 
Researchers can thus use hierarchical models alongside other approaches to understand heterogeneous effects for scholarship and policy.
\end{abstract} 


\newpage 
\doublespace 


\section{Introduction}


% one: het effects matter
Whether in observational or experimental studies, every independent variable social scientists examine impacts some units differently than others. 
Common estimands aggregate heterogeneous effects.\footnote{For instance, \citet{Abramsonetal2022} note that the average marginal component effect (AMCE) of conjoint experiments gives more weight to intense preferences.} 
These average effects are useful, but they often obscure interesting and important variation. 


As a result, understanding heterogeneous effects is essential for policy and scholarship. 
Estimating heterogeneity allows scholars to clarify the connection between their independent variable and outcome.
Policymakers can maximize the impact of finite resources with targeted interventions, for example by providing job training to individuals who are more likely to benefit. 
% expand/sharpen this, if there is space. (There is not for PA) 


% two: introduce my solution 
This paper explains why, when and how to use hierarchical models to estimate heterogeneous effects. 
A large statistics literature suggests that Bayesian hierarchical models are a useful tool for  heterogeneous effects estimation (e.g., \cite{FellerGelman2015, McElreath2016, Dorieetal2022}).
Political scientists tend to rely on interactions or machine learning tools instead, however.  
For instance, of the three applied political science citations of \citet{FellerGelman2015}, only \citet{Marquardt2022} models treatment effects. 


% power problems
The main advantage of hierarchical modeling is regularization, as partial pooling pulls estimates towards an overall mean. 
This matters because many political science applications have low power even for main effects \citep{ArelBundocketal2022}.
Adequately powered estimates of even a single interaction often requires significantly more data \citep{Gelman2018}, which can be expensive or unavailable.
As a result, statistically significant heterogeneous effect estimates may be far too large--- the result of noise in the data, not systematic differences. 
This problem appears most prominently in problems replicating findings based on interactions \citep{Simmonsetal2011}. 


At the same time, social scientists often posit conditional theories, and are interested in how different groups respond to the same stimulus for normative or policy reasons.
These understandable tendencies exacerbate the power challenges of using linear regression to estimate heterogeneous effects, because accumulating singe-modifier arguments and interest in diverse subgroups suggest that multiple modifiers matter.  
For example, scholarship on audience costs has considered how foreign policy dispositions \citep{KertzerBrutger2016}, partisanship \citep{LevenduskyHorowitz2012}, gender \citep{Barnhartetal2020, SchwartzBlair2020} and policy preferences \citep{Chaudoin2014} all modify individual reactions to a leader backing down from a threat.


% three: loads of techniques
Hierarchical modeling of heterogeneous effects addresses these concerns, and thereby fills a gap between interactions and machine learning.\footnote{\citet{BlackwellOlson2022} describe a lasso approach to interactions that sits between machine learning and linear regressions.}
Parametric interactions and subgroup analyses are ubiquitous because they are easy to interpret, but these are subject to power concerns. 
More recent work employs random forests \citep{GreenKern2012, WagerAthey2018}, support vector machines \citep{ImaiRatkovic2013}, and ensemble methods \citep{Grimmeretal2017, Kuenzeletal2019, Dorieetal2022}.
These machine learning algorithms often have some regularization component and can discover complex patterns and high-dimensional variation, but can be difficult to interpret and implement, especially in smaller social science datasets.

 
Using a hierarchical model is more flexible than parametric interactions but easier to implement and interpret than machine learning approaches.  
It preserves a simple and interpretable structure, while accommodating more factors and ameliorating the downsides of subgroup analysis. 
This facilitates argument testing.
Unlike machine learning, the hierarchical approach lacks the flexibility to discover high-dimensional heterogeneity, however.  


Hierarchical modeling therefore works best when there are more than two modifying factors and therefore many subgroups of interest, as well as less emphasis on discovery. 
These models are best at capturing variation across groups and levels when there are multiple potential modifiers.
This also works well when researchers have a clear sense of the relevant groups.\footnote{\citep{Goplerud2021} introduces a model that uses Bayesian structured sparsity to estimate which group coefficients are similar and which are different. In this approach, researchers use theory to inform potential groups, but common estimates for groups are data driven.} 


There are two key steps when theory and data make using hierarchical models worthwhile.
First, researchers should define groups based on potential sources of heterogeneity such as other treatments, context, demographics, or policy concerns. 
Second, they should estimate heterogeneous effects across groups using a hierarchical model with varying slopes and intercepts for every unique group. 
Modeling heterogeneous effects in this way produces interpretable results, which facilitates argument testing.
It also allows researchers to examine effects within groups, compare different sources of heterogeneous effects and describe how much an effect varies.  


While frequentist estimation of hierarchical models is possible, Bayesian estimation is straightforward and more informative.
Bayesian estimation provides crucial information by connecting parameters through common prior distributions, thereby regularizing estimates and propagating uncertainty. 
Working with posterior distributions also gives researchers more flexibility to describe how and when effects vary. 
While computation and coding were once a barrier to employing Bayesian methods, fitting a wide range of hierarchical models is straightforward with the brms package in \textsf{R} \citep{Buerkner2017}.\footnote{I provide example code in this note and the appendix.}
Calculating substantive effects is also simple \citep{ArelBundockme}.



% wrap and introduce the application 
In the remainder of this paper, I describe how and when to estimate hierarchical models of heterogeneous effects.
I then employ a simulation study to compare OLS and hierarchical estimates of subgroup treatment effects in multiple plausible conditions.
Finally, I demonstrate the process by analyzing a study of how military alliances shape public support for war by \citet{TomzWeeks2021}. 
The reanalysis reveals that alliances increase support for intervention most among white men who support international engagement but are otherwise skeptical of using force. 
%Alliances increase mass support for war by impacting individuals who otherwise prefer peaceful collaboration. 



\section{Hierarchical Modeling of Heterogeneous Effects}


There are two steps in hierarchical models of heterogeneous effects. 
First, researchers must define the groups over which an independent variable's impact changes. 
Researchers should create groups based on what variation is most important and interesting. 
Theory, policy concerns, or normative factors are all possible motivations. 
These set the unique combinations of characteristics such as other treatments, context and demographics determine groups.


Setting groups is the most important task, because it determines what heterogeneous effects a researcher estimates. 
Defining groups before model fitting defines what variation is most important, links heterogeneous effects to theory, and structures modeling.\footnote{It also facilitates pre-registration when applicable.}
Defining groups poorly will obfuscate results and can hinder model fitting.
   


% three ways to set groups
There are three general approaches to defining groups.  
First, researchers can set groups using combinations of other treatments, especially when an intervention has several dimensions but theory emphasizes one of them. 
The experimental design determines groups, and the model estimates heterogeneous treatment effects.   
If researchers want to know how different issues shape the impact of elite foreign policy cues \citep{GuisingerSaunders2017}, they could define groups by issues, for instance.
Hierarchical estimators for topic-sampling experiments estimate how a treatment effect varies across different topics \citep{CliffordRainey2023}. 
Researchers sometime use fully crossed regression interactions to estimate the impact of a treatment across experimental strata, but this approach risks spurious results. 


A second approach uses unit, demographic and contextual factors to create groups and estimate  effect heterogeneity. 
Here, researchers examine what factors within or around units shape their response to an independent variable.
For example, \citet{Alley2021isq} uses alliance characteristics to examine when alliance membership increases or decreases military spending.
Other use cases include estimating how different demographic groups or geographic units respond to an intervention.
One might examine how the impact of a national-level intervention varied across states, for example.\footnote{Extrapolation to a representative sample for such units might require poststratification.}


Third, researchers might use hierarchical models to address specific policy concerns.
Policy analysts often want to know how an intervention impacts a specific population. 
Researchers might want to know if a job-training program improves employment outcomes for black women in the South, for instance.  


% grouping factors: numbers 
Whether researchers use other treatments, context, or policy to determine groups, group composition should depend first on the research question.
There are some practical constraints, however.
Using too many factors can lead to model fitting and interpretation problems by creating many small groups.
How many factors is too many depends especially on the data- some datasets can support reasonably large groups for many factors. 
At the other extreme, using only one grouping factor in a hierarchical model will usually add relatively little value compared to interactions. 


After defining groups, the second step is fitting a hierarchical model of effects within groups.
The model employs a mix of varying intercepts and slopes to estimate the impact of an intervention in each group.\footnote{If other units such as states define the groups, rather than combinations of modifying variables, then adding group-level predictors is essential. For examine, in a model where an effect varies by state, adding state-level variables like ideology, population and GDP would avoid partially pooling small groups too far towards the overall mean.}
Varying slopes express how the effect of interest varies across groups, while the intercepts capture differences in levels across groups. 


For ease of exposition, consider making between-unit comparisons based on an experimental treatment.    
Start with \textit{N} units indexed by \textit{i}, some of which receive a binary treatment \textit{T}.
Assume that the outcome variable ${y}$ is normally distributed with mean $\mu_i$ and standard deviation $\sigma$.\footnote{Researchers should use binary, categorical and other outcome likelihoods.}
\textit{g} indexes two researcher-defined groups, \textit{g1} and \textit{g2}.


%The first equation predicts the outcome mean. 
The outcome for each unit is then a function of group varying intercepts $\alpha_g$, an optional matrix of control variables \textbf{X}, and a set of group treatment effects $\theta_g$.
The researcher divides all units into \textit{g} groups based on unique combinations of heterogeneous effect predictors \textbf{Z}. 
Each $\theta$ parameter estimates how the treatment effect varies across the values of  each variable.  
To capture correlations between the random intercepts and varying slopes $\rho \sigma_\alpha \sigma_\theta$, these variables should have a common multivariate normal prior.


\begin{align*}
y_i &\sim N(\mu_i, \sigma) &\text{(Likelihood)} \\
\mu_i &= \alpha + \alpha_{g1} + \theta_{g1} \textit{T} + \alpha_{g2} + \theta_{g2} \textit{T} + \textbf{X} \beta &\text{(Outcome Equation)}  \\ 
\begin{pmatrix} 
\alpha_{g} \\
\theta_{g} \\
\end{pmatrix} &\sim  N
\begin{bmatrix}
\begin{pmatrix}
\mu^\alpha_{g} \\
\mu^\theta_{g} \\
\end{pmatrix}\!\!,&
\begin{pmatrix}
\sigma^2_\alpha & \rho \sigma^\alpha_{g} \sigma^\theta_{g} \\
\rho \sigma^\alpha_{g} \sigma^\theta_{g} & \sigma^2_\theta \\
\end{pmatrix}
\end{bmatrix} & \text{(Common Prior)} \\ 
%\begin{pmatrix} 
%\alpha_{g1} \\
%\theta_{g1} \\
%\end{pmatrix} &\sim  N
%\begin{bmatrix}
%\begin{pmatrix}
%\mu^\alpha_{g2} \\
%\mu^\theta_{g2} \\
%\end{pmatrix}\!\!,&
%\begin{pmatrix}
%\sigma^2_\alpha & \rho \sigma^\alpha_{g2} \sigma^\theta_{g2} \\
%\rho \sigma^\alpha_{g2} \sigma^\theta_{g2} & \sigma^2_\theta \\
%\end{pmatrix}
%\end{bmatrix} & \text{(Common Prior: G2)} \\ 
%\begin{pmatrix} 
%\alpha_{g1*g2} \\
%\theta_{g1*g2} \\
%\end{pmatrix} &\sim  N
%\begin{bmatrix}
%\begin{pmatrix}
%\mu^\alpha_{g1*g2} \\
%\mu^\theta_{g1*g2} \\
%\end{pmatrix}\!\!,&
%\begin{pmatrix}
%\sigma^2_\alpha & \rho \sigma^\alpha_{g1*g2} \sigma^\theta_{g1*g2} \\
%\rho \sigma^\alpha_{g1*g2} \sigma^\theta_{g1*g2} & \sigma^2_\theta \\
%\end{pmatrix}
%\end{bmatrix} & \text{(Common Prior: g1*g1)} \\ 
%\theta_g &= \theta_{g1} \textit{G1} + \theta_{g2} \textit{G2} & \text{(Group Slopes)} \\
%\alpha_g &= \alpha_{g1} + \alpha_{g2} & \text{(Group Intercepts)} \\
%\mu_\theta &= \lambda_0 + \textbf{Z} \lambda &\text{(Heterogeneous Effects)} 
\end{align*}


This approach lets slopes vary across groups. 
This is a relatively simple model.\footnote{In brms for a model with no controls and two groups modifying the impact of a treatment, the model formula is y $\sim$ 1 + (1 + treat | group1 + group2)}  


The slopes and intercepts can also be nested via interactions of grouping variables. 
In that instance, the net impact of the treatment in each group depends on the linear combination of slopes in that group, specifically the $\theta$ parameters at the specific values of each variable. 
Similarly, the group intercepts would be the sum of the corresponding random intercepts for each grouping variable.


%The heterogeneous effects equation then predicts the treatment effects with the matrix \textbf{Z}, which contains unique combinations of whatever variables define the groups.  
%As a result, each $\theta$ reflects a unique mix of factors that modify the treatment.
%The second equation also includes an intercept $\lambda_0$ that estimates the impact of treatment when all sources of heterogeneity are zero.\footnote{In brms for a model with no controls and two variables modifying the impact of a treatment, the model formula is y $\sim$ 1 + treat*(var1 + var2) + (1 + treat | var1:var2). treat*(var1 + var2) expresses part of the second equation, while (1 + treat | var1:var2) lets slopes vary by group.}


The above model can be fit with Bayesian or frequentist methods, but Bayesian estimation offers important advantages.
First, it is more flexible, and including prior information can facilitate model fitting and convergence. 
Priors also help regularize estimates by pulling extreme groups towards the overall mean.
Working with posterior distributions also also provides a wealth of information about effect heterogeneity and propagates uncertainty.  


% Interpretation 
In interpreting these estimates, researchers should leverage the full range of information from the different parameters. 
First, the $\theta$ posteriors give the impact of a variable within each group.
All $\theta$s reflect a systematic component from the predictors in $\mu_\theta^g$ and a random variation in slopes from $\sigma_\theta$. 
Plotting variation in the $\theta$ estimates can give a sense of how much the estimates shift across groups. 
Other techniques such as interactions in OLS with robust standard errors provide less information.


\section{When to Use Hierarchical Models}

% advantages 
In deciding whether to use a hierarchical model, researchers must weigh specific advantages and disadvantages. 
In general, estimating heterogeneous effects in this way has three advantages.
First, researchers can make detailed inferences about heterogeneous effects in an interpretable framework. 
This helps examine theories that predict how an effect varies and compare sources of variation.\footnote{Rescaling variables in the heterogeneous effects equation can aid model fitting and coefficient comparisons \citep{Gelman2008}.} 
Partial pooling also facilitates reasonable estimates for small groups by sharing information across groups and incorporating predictors in the heterogeneous effects equation. 
Finally, this approach will be faster than machine learning approaches for many datasets as well as easier to use in small datasets.


% disadvantages
Like all methods, the hierarchical approach has downsides, some of which can be ameliorated with modifications, while others should lead researchers to use different tools. 
First, defining many small groups, perhaps by grouping based on a continuous variable, will likely lead to model fitting problems.
If using continuous variables hinders model convergence, researchers can bin continuous variables.
In general, fitting a model with many small groups will make it harder to fit a hierarchical model. 


Furthermore, hierarchical models can show general trends, but will not make powerful comparisons between every group. 
Researchers who want to compare specific groups may lack empirical leverage, especially for small groups.
This downside can also apply to other methods, however. 


% Relative to interactions
With these considerations in mind, when should researchers use hierarchical models in place of interactions?
If only one factor modifies an effect, interactions are best, as the extra information hierarchical models provide is less valuable. 
Especially if the groups are of similar size and there is one grouping factor, regularization and OLS will give similar answers.


% two modifiers
With two or more modifiers, hierarchical models begin to add value beyond. 
Interpreting triple interactions between a variable and two modifiers is challenging. 
The advantages of hierarchical modeling increase with the number of modifiers, until additional modifiers create small groups that complicate model fitting. 
The thresholds where the number of modifiers becomes an issue for hierarchical modeling depends on the data, as larger datasets can support more groups. 


% not as well suited for discovery 
The relative use cases of hierarchical models and machine learning are different. 
Unlike machine learning approaches, hierarchical models will not discover high-dimensional interactions. 
Researchers can add flexibility with additional interactions or non-linear specifications in either level of the model, but this requires a priori specification. 
Therefore, if researchers want to focus on flexible discovery, not testing an argument with multiple sources of treatment heterogeneity, they should rely more on machine-learning. 


In summary, researchers should continue to use interactions for single modifiers and machine learning to discover complex interactions. 
Hierarchical modeling works well when there are two or more modifiers and researchers have adequate data to support an informative model.  
\autoref{tab:tools-det} summarizes some relevant characteristics of hierarchical, interaction and machine learning approaches to heterogeneous effects. 
Hierarchical modeling is thus an intermediate tool between interactions and machine-learning, where researchers need more flexibility than interactions but are not willing or able to tackle the computational and interpretation challenges of machine learning. 


\begin{table}
\begin{tabular}{|p{1in}|p{1.5in}|p{1.5in}|p{1.5in}|} \hline
                 & Hierarchical Models & Interactions/Subgroup & Machine Learning \\
\hline
Factors              & Two or more          & One or two         & Many \\ \hline
Sample Size          & Conditional on number of factors            & Medium to large, depending on main effect size    & Large \\ \hline
Complexity           & Medium             & Low                & High \\ \hline
Computational Cost   & Medium or High             & Low                & High \\ \hline
Interpretability     & High               & High               & Low \\ \hline
Modifiers            & Specified          & Specified      & Discovered or Specified \\
\hline
\end{tabular}
\caption{Key characteristics of different approaches to estimating heterogeneous effects.}
\label{tab:tools-det}
\end{table}


\section{Performance on Simulated Data}

To assess how hierarchical models compare to OLS interactions, I first assess their performance on simulated data. 
The first simulation varies the number of groups and variation in the treatment effect across groups. 
To second simulation varies the sample size and variation in treatment effect, while leaving the number of groups constant.


In the first simulation, I define between 2 and 10 groups in a dataset with 1,000 observations.
Each group has the same number of data points, so as the number of groups increases, the number of observations in each group falls. 
More and smaller groups will likely benefit more from regularization, improving the performance of hierarchical modeling.


I base the group treatment effects on interactions between the treatment and a dummy indicator for each group. 
I simulate the coefficents by drawing them from a normal distribution with a mean of 0 and standard deviation of either .05, .25 or .75. 
Increasing the standard deviation adds greater variation to the group-level effects.
This captures the degree of heterogeneity in the relationship.


The interactions predict the mean of the outcome variable, $\mu_y$, which has a standard deviation of .25. 
The key treatment effect is the difference in $\mu_y$ between treatment and control observations within each group. 
I use differences in $mu_y$ because this is the systematic component of the simulation- the observed outcome $y$ has a random component. 


I then fit two models. 
First, I fit an OLS model that uses interactions of the grouping dummies with the treatment variable.
Next, I fit a hierarchical model that estimates varying intercepts and treatment slopes across groups.


There are several potential metrics for comparing performance. 
One is bias; the gap between the estimated treatment effects in each group and the true treatment effects. 
Hierarchical models are sometimes biased on average, however \citep{CliffordRainey2024}, and the question is whether reduced variance in the estimates offsets bias enough to improve the estimates.
To assess that, I compare the root mean squared error of the group treatment effect estimates for each model. 


%\begin{figure}[htpb]
%	\centering
%		\includegraphics[width=0.95\textwidth]{../figures/sim-rmse-coef.png}
%	\caption{.}
%	\label{fig:sim-rmse-coef}
%\end{figure}


\section{Example Application: Alliances and Public Support for War} 


In the following, I further demonstrate how the hierarchical approach works and the benefits of regularizing effect estimates by reanalyzing a study by \citet{TomzWeeks2021} (TW hereafter). 
TW examine whether the public is more willing to go to war for an allied country.
In a factorial experiment with vignettes, they find a 33\% average increase in support for military intervention on behalf of another country if that country is an ally. 
This is a large and potentially important relationship. 


Given the size of the main effect, TW's paper is in some ways a best case scenario for comparing interactions and hierarchical models. 
Corresponding interaction effects may be quite large, and their sample size of 1,200 respondents is not unusual in published work. 
At the same time, TW estimated an array of interactions to check how other treatments modify the impact of alliances.
There are 64 unique treatment groups with anywhere from 11 to 32 respondents in each, so estimates of the impact of alliances in the 32 pairs of alliance treatment and control groups employ at most 54 data points. 
As such, employing varying slopes for regularization will likely offer substantial benefits. 
I document these gains first by analyzing the how other experimental treatments modify the impact of alliances, and then exploring demographic differences by alliance. 



\subsection{Differences by Experimental Scenario}


Along with alliances, TW randomly assign whether the potential beneficiary of U.S. intervention is a democracy or not, the stakes of intervention, the potential costs, and the region of the world. 
They estimate the impact of alliances in the 32 treatment conditions with an OLS model that fully crosses interactions between the treatments, and calculate marginal effects that average over these groups. 
I keep the same fully crossed structure in the treatment interactions to define groups, but use a varying slopes model to estimate the impact of alliances in each treatment group.


\begin{figure}[htpb]
	\centering
		\includegraphics[width=0.95\textwidth]{../figures/tw-het-treat-comp.png}
	\caption{Comparison of OLS and hierarchical estimates of the impact of alliance across experimental conditions. Estimates divided based on the regional treatment variable for ease of presentation.}
	\label{fig:tw-het-treat-comp}
\end{figure}


\autoref{fig:tw-het-treat-comp} compares the estimated alliance treatment effects across the experimental groups with the OLS and hierarchical models.
Two aspects of \autoref{fig:tw-het-treat-comp} show the regularization benefits of hierarchical modeling.
First, the hierarchical estimates are more precise. 
The credible intervals in the hierarchical model are smaller because they draw on information from every group. 


Second, the hierarchical estimates are less variable. 
This reduces the estimated variation in how alliances impact mass attitudes, and is obvious if we compare estimates within regions. 
The OLS interactions are more dispersed, while the hierarchical estimates hew more closely to the overall mean.
This is especially notable in the African and Latin American scenarios. 
Inasmuch as differences across scenarios are driven by noise in small treatment groups, the hierarchical model smooths out some of that random variation.



\subsection{Who Responds to Alliances}

I used race, gender, hawkishness and internationalism to define demographic groups across which the impact of alliances might vary. 
I selected these variables because foreign policy dispositions like militant assertiveness shape general willingness to use force \citep{Kertzeretal2014} as do gender \citep{Barnhartetal2020} and race. 
I also control for other experimental manipulations.\footnote{See the appendix for priors.} 
Following TW's OLS analysis, I use a Gaussian likelihood, although the outcome is a binary variable. 


I describe the results in two steps. 
First, I summarize the distribution of alliance effects. 
After this, I summarize the sources of variation in the alliance effect in \autoref{fig:tw-het-source} and present the resulting heterogeneous effects for every group in \autoref{fig:tw-treat-het}.

\begin{figure}[htpb]
	\centering
		\includegraphics[width=0.95\textwidth]{../figures/tw-treat-het-sum.png}
	\caption{Posterior distribution of all estimated impacts of alliances on support for using force. Text values give notable point estimates, and parentheses summarize the 95\% credible interval.}
	\label{fig:tw-treat-het-sum}
\end{figure}


How alliances impact support for using force varies widely. 
\autoref{fig:tw-treat-het-sum} provides an initial summary of that variation, and highlights several noteworthy estimates. 


First, \autoref{fig:tw-treat-het-sum} notes that the minimum estimated impact of an alliance on a demographic group is .05, while the maximum is .53. 
The maximum effect occurs among white men with high internationalism and low hawkishness.
The minimum effect applies to non-white women with low internationalism and high hawkishness. 
There is no overlap in the posteriors of these estimates. 
The median group treatment effect estimate is .31, and this group of respondents is non-white men with middling internationalism and hawkishness. 
Alliances never clearly decrease support for intervention, but how much they increase support varies widely. 


\autoref{fig:tw-treat-het-sum} also presents the variation in how alliances impact demographic groups.
The standard deviation of all posterior draws is .13. 
Roughly 5\% of variation in the alliance effect is not explained by systematic regression components in \autoref{fig:tw-het-source}.\footnote{This is $\sigma_\theta$ above.}


\autoref{fig:tw-het-source} plots how the impact of alliances varies across support for international engagement, willingness to use force, race and gender. 
These variables define the groups, so differences in the alliance slope follows their individual and joint variation. 
Because there are many groups, the impact of alliances varies widely within each level of these variables, but there are some clear patterns. 


\begin{figure}[htpb]
	\centering
		\includegraphics[width=0.95\textwidth]{../figures/tw-het-source.png}
	\caption{Variation in the impact of alliances on support for military intervention across four variables that set groups. Each point marks the impact of alliances on a specific group, and boxplots summarize the median and interquartile range of the slopes within each level of the variable. All slopes are present in each facet.}
	\label{fig:tw-het-source}
\end{figure}


Individuals with minimal interest in international engagement are less responsive to alliances, while any greater support for internationalism leads to a fairly consistent response to alliances. 
Similarly, alliances exert less impact on individuals who have minimal militant assertiveness. 
Alliances are very influential for individuals with low but greater than minimal hawkishness, however. 
Among those with moderate or high hawkishness, alliances have a fairly consistent impact. 
The media alliance impact is also greater for men, and among white respondents. 
Letting slopes vary across each level of the grouping variables generates more flexibility, and clearly shows the difference between individuals with very low internationalism or low militant assertiveness and others. 


As \autoref{fig:tw-het-source} suggests, alliances increase support for foreign intervention most among white men, especially those with low hawkishness and some internationalism.
By contrast, alliances have little impact on support for war among non-white females who are also skeptical of international engagement and unwilling to use force.
Individuals with more ambivalent foreign policy views respond more typically to TW's alliance treatment. 


Finally, I illustrate the regularization benefits of a hierarchical model in \autoref{fig:tw-het-treat-comp}. 
Again, the hierarchical estimates are more precise and less variable than OLS with fully crossed interactions. 
This occurs because the model partially pools information across groups, which reduces uncertainty and pulls the estimated impact of an alliance towards the overall mean.


\begin{figure}[htpb]
	\centering
		\includegraphics[width=0.95\textwidth]{../figures/tw-treat-het-comp.png}
	\caption{Estimated impact of alliance on support for military intervention in different subgroups of respondents. Groups defined based on fully cross interactions between the demographic variables. Error bars give the 95\% credible interval in the hierarchical model and 95\% confidence interval in the OLS interactions model.}
	\label{fig:tw-treat-het-comp}
\end{figure}


The other noteworthy concern is that OLS estimation with fully crossed interactions can engage in wild extrapolations.
In some groups, the estimated impact of an alliance rises to 80 or 90\%. 
This reflects linear interactions that might easily be picking up noise in the data.


All these estimates suggest that internationalism matters more than than hawkishness for understanding who is willing to fight for U.S. allies. 
Alliances may impact hawks less because these individuals support intervention regardless. 
Military alliances matter most to backers of international engagement who less willing to use force, but not entirely averse to military intervention.


% variation
These results show some of the strengths and weaknesses of the hierarchical approach to heterogeneous effects.\footnote{In the appendix, I analyze \citet{BushPrather2020}.}
A simple model based on demographic groups provides precise insights about who heeds alliances in supporting using force abroad. 
At the same time, because some demographic groups are small, powerful comparisons between most groups is challenging. 
Fewer groups would have more data and less uncertainty but perhaps obscure variation across key demographic characteristics. 


\section{Conclusion}


This note explained how and when to use hierarchical models to estimate heterogeneous effects. 
Bayesian modeling can apply to a wide range of outcomes, data structures, and theories. 
It also details what drives variation in an effect and how much an effect varies. 
Explicitly modeling how different groups respond to an independent variable can help test arguments and inform policy.  


Hierarchical modeling provides an intermediate approach between interactions or subgroup analyses and machine learning algorithms. 
For interactions with one or two variables, relying on simple interaction tools is best. 
Similarly, machine learning is best for discovery of complex heterogeneity.
When there are two or more modifiers and many groups of theoretical interest, hierarchical modeling allows theoretically informed and interpretable estimation of effect variation. 

As a result, hierarchical modeling complements existing tools and should not replace them. 
Researchers can use hierarchical models to check and inform other techniques, for instance by seeing if a key interaction holds when there are multiple modifiers, or comparing multiple modifiers that past theories have identified. 
Using hierarchical modeling can thus help scholars and policymakers better understand heterogeneous effects.


\section*{Acknowledgements}

Thanks to Taylor Kinsley Chewning, Andrew Gelman and Carlisle Rainey for helpful comments.

\singlespace
\bibliography{../../MasterBibliography} 


%\input{../appendix/appendix.tex}

\end{document}

